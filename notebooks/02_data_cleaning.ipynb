{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a629edb-694b-4608-bc43-71eb19646da6",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "## Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a75a7d19-59ce-4e58-a288-67d8894490b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7502da22a5dd4e0a8a8d3ead2d74c712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading CSV in chunks:   0%|          | 0/297 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[1;32m     15\u001b[0m file_path \u001b[38;5;241m=\u001b[39m paths_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpaths\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_data\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_explored.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 16\u001b[0m df \u001b[38;5;241m=\u001b[39m read_csv_with_progress(file_path, dtypes\u001b[38;5;241m=\u001b[39mdtypes_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtypes\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Analyze missing values\u001b[39;00m\n\u001b[1;32m     19\u001b[0m missing_data \u001b[38;5;241m=\u001b[39m analyze_missing_values(df)\n",
      "File \u001b[0;32m~/Documents/DataScienceAndMachineLearning/Rex_Coleman_Machine_Learning_Cybersecurity_Portfolio/Microsoft_Malware_Prediction_Project/src/data/load_data.py:82\u001b[0m, in \u001b[0;36mread_csv_with_progress\u001b[0;34m(file_path, chunk_size, dtypes)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mnum_chunks, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading CSV in chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, chunksize\u001b[38;5;241m=\u001b[39mchunk_size, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 82\u001b[0m         df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, chunk], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     83\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtypes:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Microsoft_Malware_Prediction/lib/python3.11/site-packages/pandas/core/reshape/concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    383\u001b[0m     objs,\n\u001b[1;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    393\u001b[0m )\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Microsoft_Malware_Prediction/lib/python3.11/site-packages/pandas/core/reshape/concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[1;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[0;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m concatenate_managers(\n\u001b[1;32m    685\u001b[0m     mgrs_indexers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_axes, concat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm_axis, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m    686\u001b[0m )\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Microsoft_Malware_Prediction/lib/python3.11/site-packages/pandas/core/internals/concat.py:177\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m    167\u001b[0m vals \u001b[38;5;241m=\u001b[39m [ju\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m ju \u001b[38;5;129;01min\u001b[39;00m join_units]\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_extension:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# _is_uniform_join_units ensures a single dtype, so\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m#  we can use np.concatenate, which is more performant\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# expected \"Union[_SupportsArray[dtype[Any]],\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;66;03m# _NestedSequence[_SupportsArray[dtype[Any]]]]\"\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(vals, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_1d_only_ea_dtype(blk\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     values \u001b[38;5;241m=\u001b[39m concat_compat(vals, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ea_compat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from src.data.clean_data import handle_missing_values, remove_duplicates, analyze_missing_values, compute_correlations, visualize_distributions\n",
    "from src.data.load_data import read_csv_with_progress\n",
    "from src.tests.test_clean_data import check_no_missing_values, check_no_duplicates\n",
    "from src.config_loader import paths_config, dtypes_config, preprocessing_config\n",
    "\n",
    "# Load the data\n",
    "file_path = paths_config['paths']['processed_data'] + 'train_explored.csv'\n",
    "df = read_csv_with_progress(file_path, dtypes=dtypes_config['dtypes'])\n",
    "\n",
    "# Analyze missing values\n",
    "missing_data = analyze_missing_values(df)\n",
    "print(missing_data.head(20))\n",
    "\n",
    "# Visualize distributions of features with the most missing values\n",
    "top_missing_features = missing_data.head(10).index.tolist()\n",
    "visualize_distributions(df, top_missing_features)\n",
    "\n",
    "# Compute correlations with the target variable\n",
    "correlations = compute_correlations(df, 'HasDetections')\n",
    "print(correlations.head(20))\n",
    "\n",
    "# Handle missing values based on strategy from preprocessing config\n",
    "df = handle_missing_values(df, strategy=preprocessing_config['preprocessing']['missing_values'])\n",
    "\n",
    "# Remove duplicates\n",
    "df = remove_duplicates(df, 'MachineIdentifier')\n",
    "\n",
    "# Perform tests\n",
    "print(check_no_missing_values(df))  # Should print True\n",
    "print(check_no_duplicates(df, 'MachineIdentifier'))  # Should print True\n",
    "\n",
    "# Save the cleaned dataframe for subsequent steps\n",
    "chunk_size = 100000\n",
    "num_chunks = len(df) // chunk_size + 1\n",
    "\n",
    "with tqdm(total=num_chunks, desc=\"Saving cleaned CSV in chunks\") as pbar:\n",
    "    for i, start in enumerate(range(0, len(df), chunk_size)):\n",
    "        end = start + chunk_size\n",
    "        df[start:end].to_csv(paths_config['paths']['processed_data'] + 'train_cleaned.csv', mode='a', index=False, header=(i==0))\n",
    "        pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c3ec20-ea34-41f7-9256-efbe4baf8402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c85a361-1684-4db5-8a4c-0daebfcdece5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34de9b28-9c39-4e3f-a753-b0a53e387a63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be13d2b-d682-4cce-beda-3f1251d1167a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc615b37-034e-4685-a513-33903bf11685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942a517c20604f7babfffb48d0134514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading CSV in chunks:   0%|          | 0/208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has 60686127 missing values.\n",
      "False\n",
      "True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4de7a209ba5436ba1a05404965e0909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving cleaned CSV in chunks:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from src.data.clean_data import handle_missing_values, remove_duplicates\n",
    "from src.tests.test_clean_data import check_no_missing_values, check_no_duplicates\n",
    "from src.data.load_data import read_csv_with_progress, load_dtypes_from_yaml, read_config\n",
    "\n",
    "# Load configuration\n",
    "config = read_config()\n",
    "file_path = config['paths']['processed_data'] + 'train_explored.csv'\n",
    "\n",
    "# Load the data\n",
    "df = read_csv_with_progress(file_path)\n",
    "\n",
    "# Handle missing values\n",
    "df = handle_missing_values(df)\n",
    "\n",
    "# Remove duplicates\n",
    "df = remove_duplicates(df, 'MachineIdentifier')\n",
    "\n",
    "# Perform tests\n",
    "print(check_no_missing_values(df))  # Should print True\n",
    "print(check_no_duplicates(df, 'MachineIdentifier'))  # Should print True\n",
    "\n",
    "# Save the cleaned dataframe for subsequent steps\n",
    "chunk_size = 100000\n",
    "num_chunks = len(df) // chunk_size + 1\n",
    "\n",
    "with tqdm(total=num_chunks, desc=\"Saving cleaned CSV in chunks\") as pbar:\n",
    "    for i, start in enumerate(range(0, len(df), chunk_size)):\n",
    "        end = start + chunk_size\n",
    "        df[start:end].to_csv(config['paths']['processed_data'] + 'train_cleaned.csv', mode='a', index=False, header=(i==0))\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7729e7-95b4-442d-9256-290ebb896541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a3fa67-77c9-4675-a1fb-23239c496ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from src.data.clean_data import handle_missing_values, remove_duplicates\n",
    "from src.tests.test_clean_data import check_no_missing_values, check_no_duplicates\n",
    "from src.data.load_data import read_csv_with_progress, load_dtypes_from_yaml, read_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35f5eae-f022-4c4b-8713-cacb670038ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = read_config()\n",
    "file_path = config['paths']['processed_data'] + 'train_explored.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc2db75-e0eb-4149-8678-0da1e3288938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = read_csv_with_progress(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb003434-328f-467a-8ea3-3e83e83a51ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile missing values\n",
    "missing_values = df.isnull().sum().sort_values(ascending=False)\n",
    "print(\"Missing values in each column:\\n\", missing_values.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a83f6-0604-47d1-a66c-f68ce2ffee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, save the missing values profile to a CSV for detailed analysis\n",
    "missing_values.to_csv(config['paths']['processed_data'] + 'missing_values_profile.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7abe13-4835-45e5-853d-303b46008169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "df = handle_missing_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7018ab-2198-4e5e-a634-862ddc79a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df = remove_duplicates(df, 'MachineIdentifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2dd77e-73b7-40b1-9193-019542837684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further cleaning steps can be added here, such as removing or imputing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835776d9-7b6c-4af6-8e47-04dd87a56609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform tests\n",
    "print(check_no_missing_values(df))  # Should print True\n",
    "print(check_no_duplicates(df, 'MachineIdentifier'))  # Should print True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f872385-fe0d-4dfd-9145-be6b1286a87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataframe for subsequent steps\n",
    "chunk_size = 100000\n",
    "num_chunks = len(df) // chunk_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20db3971-6356-4f4c-af57-0e0c2909792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=num_chunks, desc=\"Saving cleaned CSV in chunks\") as pbar:\n",
    "    for i, start in enumerate(range(0, len(df), chunk_size)):\n",
    "        end = start + chunk_size\n",
    "        df[start:end].to_csv(config['paths']['processed_data'] + 'train_cleaned.csv', mode='a', index=False, header=(i==0))\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecf08ab-ef0e-4fd8-bc21-7dfc1b6762ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
