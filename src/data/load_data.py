import pandas as pd
import dask.dataframe as dd
from tqdm.notebook import tqdm
import logging
import yaml

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def load_data(file_path, dtypes, use_dask=False, chunk_size=100000):
    """
    Load data from a CSV file with a progress bar.
    
    Parameters:
    - file_path: str, path to the CSV file.
    - dtypes: dict, dictionary specifying data types.
    - use_dask: bool, whether to use dask for large data files.
    - chunk_size: int, size of chunks to read at a time.
    
    Returns:
    - DataFrame: loaded data.
    """
    if use_dask:
        df = dd.read_csv(file_path, dtype=dtypes).compute()
    else:
        df = read_csv_with_progress(file_path, dtypes=dtypes, chunk_size=chunk_size)
    
    logging.info(f'Successfully loaded data from {file_path}')
    return df

def read_config(config_path='../config.yaml'):
    """
    Read configuration from a YAML file.
    
    Parameters:
    - config_path: str, path to the YAML configuration file.
    
    Returns:
    - dict: configuration parameters.
    """
    with open(config_path, 'r') as file:
        config = yaml.safe_load(file)
    return config

def load_dtypes_from_yaml(yaml_file):
    """
    Load data types from a YAML configuration file.
    
    Parameters:
    - yaml_file: str, path to the YAML configuration file.
    
    Returns:
    - dict: data types configuration.
    """
    config = read_config(yaml_file)
    return config['dtypes']

def read_csv_with_progress(file_path, chunk_size=100000, dtypes=None):
    """
    Read CSV file with specified dtypes and a progress bar.
    
    Parameters:
    - file_path: str, path to the CSV file.
    - chunk_size: int, number of rows per chunk.
    - dtypes: dict, data types for the columns.
    
    Returns:
    - DataFrame: loaded data.
    """
    total_rows = sum(1 for _ in open(file_path)) - 1  # Total number of rows minus the header
    num_chunks = total_rows // chunk_size + 1
    
    df = pd.DataFrame()
    with tqdm(total=num_chunks, desc="Loading CSV in chunks") as pbar:
        for chunk in pd.read_csv(file_path, dtype=dtypes, chunksize=chunk_size, low_memory=False):
            df = pd.concat([df, chunk], ignore_index=True)
            pbar.update(1)

    return df

if __name__ == "__main__":
    from src.config_loader import paths_config, dtypes_config

    file_path = paths_config['paths']['raw_data'] + 'train.csv'
    dtypes = dtypes_config['dtypes']
    df = load_data(file_path, dtypes, use_dask=True)
    print(df.info())













# import pandas as pd
# import dask.dataframe as dd
# from tqdm.notebook import tqdm
# import logging
# import yaml

# # Set up logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


# def load_data(file_path, dtypes, use_dask=False, chunk_size=100000):
#     """
#     Load data from a CSV file with a progress bar.
    
#     Parameters:
#     - file_path: str, path to the CSV file.
#     - dtypes: dict, dictionary specifying data types.
#     - use_dask: bool, whether to use dask for large data files.
#     - chunk_size: int, size of chunks to read at a time.
    
#     Returns:
#     - DataFrame: loaded data.
#     """
#     if use_dask:
#         df = dd.read_csv(file_path, dtype=dtypes).compute()
#     else:
#         df = read_csv_with_progress(file_path, dtypes=dtypes, chunk_size=chunk_size)

#     for col, dtype in dtypes.items():
#         if dtype == 'category':
#             df[col] = df[col].astype('category')
#         elif dtype.startswith('int') or dtype.startswith('float'):
#             df[col] = df[col].astype(dtype)
    
#     logging.info(f'Successfully loaded data from {file_path}')
#     return df

# def read_config(config_path='../config.yaml'):
#     """
#     Read configuration from a YAML file.
    
#     Parameters:
#     - config_path: str, path to the configuration YAML file.
    
#     Returns:
#     - dict: configuration parameters.
#     """
#     with open(config_path, 'r') as file:
#         config = yaml.safe_load(file)
#     return config

# def load_dtypes_from_yaml(yaml_file):
#     """
#     Load data types from a YAML configuration file.
    
#     Parameters:
#     - yaml_file: str, path to the YAML configuration file.
    
#     Returns:
#     - dict: data types configuration.
#     """
#     config = read_config(yaml_file)
#     return config['dtypes']

# def read_csv_with_progress(file_path, chunk_size=100000, dtypes=None):
#     """
#     Read CSV file with a progress bar.
    
#     Parameters:
#     - file_path: str, path to the CSV file.
#     - chunk_size: int, number of rows per chunk.
#     - dtypes: dict, data types for the columns.
    
#     Returns:
#     - DataFrame: loaded data.
#     """
#     total_rows = sum(1 for _ in open(file_path)) - 1  # Total number of rows minus the header
#     num_chunks = total_rows // chunk_size + 1
    
#     df = pd.DataFrame()
#     with tqdm(total=num_chunks, desc="Loading CSV in chunks") as pbar:
#         for chunk in pd.read_csv(file_path, chunksize=chunk_size, low_memory=False):
#             df = pd.concat([df, chunk], ignore_index=True)
#             pbar.update(1)

#     if dtypes:
#         for col, dtype in dtypes.items():
#             if dtype == 'category':
#                 df[col] = df[col].astype('category')
#             elif dtype.startswith('int') or dtype.startswith('float'):
#                 df[col] = df[col].astype(dtype)

#     return df

# if __name__ == "__main__":
#     config = read_config()
#     file_path = config['paths']['raw_data']
#     dtypes = config['dtypes']
#     df = load_data(file_path, dtypes, use_dask=True)
#     print(df.head())

# def read_csv_with_dtypes(file_path, dtypes, chunk_size=100000):
#     """
#     Read CSV file with specified dtypes and a progress bar.
    
#     Parameters:
#     - file_path: str, path to the CSV file.
#     - dtypes: dict, dictionary specifying data types.
#     - chunk_size: int, size of chunks to read at a time.
    
#     Returns:
#     - DataFrame: loaded data.
#     """
#     total_rows = sum(1 for _ in open(file_path)) - 1  # Total number of rows minus the header
#     num_chunks = total_rows // chunk_size + 1
    
#     df = pd.DataFrame()
#     with tqdm(total=num_chunks, desc="Loading CSV in chunks") as pbar:
#         for chunk in pd.read_csv(file_path, dtype=dtypes, chunksize=chunk_size, low_memory=False):
#             df = pd.concat([df, chunk], ignore_index=True)
#             pbar.update(1)
    
#     logging.info(f'Successfully loaded data from {file_path}')
#     return df

# if __name__ == "__main__":
#     from src.config_loader import paths_config, dtypes_config

#     file_path = paths_config['paths']['raw_data'] + 'train.csv'
#     dtypes = dtypes_config['dtypes']
#     df = read_csv_with_dtypes(file_path, dtypes)
#     print(df.info())






# # import pandas as pd
# # import dask.dataframe as dd
# # from tqdm.notebook import tqdm
# # import logging
# # import yaml

# # # Set up logging
# # logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# # def load_data(file_path, dtypes, use_dask=False, chunk_size=100000):
# #     """
# #     Load data from a CSV file with a progress bar.
    
# #     Parameters:
# #     - file_path: str, path to the CSV file.
# #     - dtypes: dict, dictionary specifying data types.
# #     - use_dask: bool, whether to use dask for large data files.
# #     - chunk_size: int, size of chunks to read at a time.
    
# #     Returns:
# #     - DataFrame: loaded data.
# #     """
# #     if use_dask:
# #         df = dd.read_csv(file_path, dtype=dtypes).compute()
# #     else:
# #         df = read_csv_with_progress(file_path, dtypes=dtypes, chunk_size=chunk_size)

# #     for col, dtype in dtypes.items():
# #         if dtype == 'category':
# #             df[col] = df[col].astype('category')
# #         elif dtype.startswith('int') or dtype.startswith('float'):
# #             df[col] = df[col].astype(dtype)
    
# #     logging.info(f'Successfully loaded data from {file_path}')
# #     return df

# # def read_config(config_path='../config.yaml'):
# #     """
# #     Read configuration from a YAML file.
    
# #     Parameters:
# #     - config_path: str, path to the configuration YAML file.
    
# #     Returns:
# #     - dict: configuration parameters.
# #     """
# #     with open(config_path, 'r') as file:
# #         config = yaml.safe_load(file)
# #     return config

# # def load_dtypes_from_yaml(yaml_file):
# #     """
# #     Load data types from a YAML configuration file.
    
# #     Parameters:
# #     - yaml_file: str, path to the YAML configuration file.
    
# #     Returns:
# #     - dict: data types configuration.
# #     """
# #     config = read_config(yaml_file)
# #     return config['dtypes']

# # def read_csv_with_progress(file_path, chunk_size=100000, dtypes=None):
# #     """
# #     Read CSV file with a progress bar.
    
# #     Parameters:
# #     - file_path: str, path to the CSV file.
# #     - chunk_size: int, number of rows per chunk.
# #     - dtypes: dict, data types for the columns.
    
# #     Returns:
# #     - DataFrame: loaded data.
# #     """
# #     total_rows = sum(1 for _ in open(file_path)) - 1  # Total number of rows minus the header
# #     num_chunks = total_rows // chunk_size + 1
    
# #     df = pd.DataFrame()
# #     with tqdm(total=num_chunks, desc="Loading CSV in chunks") as pbar:
# #         for chunk in pd.read_csv(file_path, chunksize=chunk_size, low_memory=False):
# #             df = pd.concat([df, chunk], ignore_index=True)
# #             pbar.update(1)

# #     if dtypes:
# #         for col, dtype in dtypes.items():
# #             if dtype == 'category':
# #                 df[col] = df[col].astype('category')
# #             elif dtype.startswith('int') or dtype.startswith('float'):
# #                 df[col] = df[col].astype(dtype)

# #     return df

# # if __name__ == "__main__":
# #     config = read_config()
# #     file_path = config['paths']['raw_data']
# #     dtypes = config['dtypes']
# #     df = load_data(file_path, dtypes, use_dask=True)
# #     print(df.head())
